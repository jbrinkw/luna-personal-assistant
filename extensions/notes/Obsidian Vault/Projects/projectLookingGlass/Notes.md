8/21/25
what i can remember:
- i had to use a bigger model to capture the effect. smaller ones worked but not distictly
- i had to subtract the embedding from the next token bc the token i was trying to work with was just predicting the next token instead of just its own value
- whatever i tried with visiulation didnt work

i think im going to have trouble finding a away to translate the latent direcition from the llm to the difuser. i mean i could half ass it and just find the nearset neibor and just copy the word but i want to go in more dpeth than thatwhat i can remember:
- i had to use a bigger model to capture the effect. smaller ones worked but not distictly
- i had to subtract the embedding from the next token bc the token i was trying to work with was just predicting the next token instead of just its own value
- whatever i tried with visiulation didnt work
- i needed to find an optimal weight to for the next token subtraction

i think im going to have trouble finding a away to translate the latent direcition from the llm to the difuser. i mean i could half ass it and just find the nearset neibor and just copy the word but i want to go in more dpeth than that

im qurestioning how intresting this project is like i feel like just text part of this alone was most of the intresting part and showing that the last token in the sentance encapsuates the whole prompt. at least in my current prompt thats probaly whats happening

what is the point of this project. to vizulize how the model thinks. so far ive gotten intresting results but now that im at the vizulaizing part idrk where to take it. like yeah its cool to the man go to king but what else could i do that would be intresting?

i think a potentially more intresting project would be being able to select any token and use it as a special input in a prompt like reflect on x or tell me a story about x to see what kinda stuff the model is retaining in that token